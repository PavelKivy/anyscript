{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb0c299",
   "metadata": {},
   "source": [
    "# üí° QLoRA –¥–æ–æ–±—É—á–µ–Ω–∏–µ DeepSeek 14B Chat –Ω–∞ —Å–≤–æ–∏—Ö —Ñ–∞–π–ª–∞—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9fc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets peft accelerate bitsandbytes trl sentencepiece python-docx --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9156b2",
   "metadata": {},
   "source": [
    "## üìÅ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è .txt –∏ .docx –≤ JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from docx import Document\n",
    "\n",
    "input_dir = \"your_files\"  # –ü–∞–ø–∫–∞ —Å —Ç–≤–æ–∏–º–∏ .txt –∏ .docx\n",
    "output_path = \"data.jsonl\"\n",
    "\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \"\\n\".join(p.text for p in doc.paragraphs if p.text.strip())\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "    for fname in os.listdir(input_dir):\n",
    "        path = os.path.join(input_dir, fname)\n",
    "        if fname.endswith(\".txt\"):\n",
    "            text = open(path, encoding=\"utf-8\").read().strip()\n",
    "        elif fname.endswith(\".docx\"):\n",
    "            text = read_docx(path)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if text:\n",
    "            json.dump({\"text\": text}, out, ensure_ascii=False)\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "print(\"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9dfb7",
   "metadata": {},
   "source": [
    "## üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b281669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-llm-14b-chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=\"bfloat16\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=bnb_config, trust_remote_code=True)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\", split=\"train\")\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-lora-chat\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=2,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913f4ad",
   "metadata": {},
   "source": [
    "## üîó –°–ª–∏—è–Ω–∏–µ –≤–µ—Å–æ–≤ LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbeddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "peft_model_id = \"./deepseek-lora-chat\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\", trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(\"./merged-model\")\n",
    "tokenizer.save_pretrained(\"./merged-model\")\n",
    "print(\"‚úÖ –°–ª–∏—Ç—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./merged-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e87be7",
   "metadata": {},
   "source": [
    "## ‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ GGUF –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411f974",
   "metadata": {},
   "source": [
    "## üß± –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –≤ GGUF –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11018200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£–±–µ–¥–∏—Å—å, —á—Ç–æ —É —Ç–µ–±—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω llama.cpp\n",
    "# https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "# 1. –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ —Å–æ–±–∏—Ä–∞–µ–º llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "%cd llama.cpp\n",
    "!make -j\n",
    "\n",
    "# 2. –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –∏–∑ HuggingFace –≤ GGUF\n",
    "# –ü—É—Ç—å –¥–æ –º–æ–¥–µ–ª–∏ ‚Äî ../merged-model (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ llama.cpp)\n",
    "!python3 convert.py --outfile ../deepseek-merged.gguf --model-dir ../merged-model\n",
    "\n",
    "# 3. –§–∞–π–ª deepseek-merged.gguf –≥–æ—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Ollama\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
